#!/usr/bin/env python

import glob
import string
import os
import sys
import argparse
import logging
import fnmatch 
import string
import json

from datetime import datetime

logger = logging.getLogger (__name__)

class Util (object):
    ''' General system utilities '''

    def get_file_as_dict (self, file_name):
        d = {}
        with open (file_name) as f:
            for line in f:
                line = line.strip ()
                if '=' in line:
                    (key, val) = string.split (line, sep = '=', maxsplit = 1)
                    d [key.strip ()] = val.strip ()
        return d

    def get_timestamp (self):
        return datetime.now ().strftime ("%FT%H:%M:%SZ")

    def rm_all (self, dirname, pattern):
        logger.debug ("rm_all: %s %s", dirname, pattern)
        file_pattern = os.path.join (dirname, pattern)
        for file_name in glob.glob (file_pattern):
            logger.info ("  --Removing %s", file_name)
            os.remove (file_name)
    
    def write_json (self, obj, file_name, preamble = ''):
        with open (file_name, 'w') as stream:
            stream.write (preamble)
            stream.write (json.dumps (obj, indent = 2, sort_keys = True))


class StatBP (object):
    ''' Manages Pegasus static.bp workflow representation files '''
    def __init__ (self, dag):
        self.u = Util ()
        self.dag = dag
        logger.info ("statbp: %s", self.dag)
        self.wf = self.dag.replace ('.dag', '')
        self.task = []
        self.task_edge = []
        self.job = []
        self.job_edge = []
        self.job_map = []        
            
        self.task_text = ''.join ([ 'ts={0} event=task.info xwf.id={1} task.id="{2}" type="1" ',
                                    'type_desc="compute" transformation="{3}::{2}:1.0" argv=""' ])
        
        self.job_text = ''.join ([ 'ts={0} event=job.info xwf.id={1} job.id="{2}" type="1" submit_file="{3}" ',
                                   'type_desc="compute" clustered="0" max_retries="3" ',
                                   'executable="{4}" argv="{5}" task_count="0"' ])
        
        self.map_text = 'ts={0} event=wf.map.task_job xwf.id={1} job.id="{2}" task.id="{3}"'
            
        self.task_edge_text = 'ts={0} event=task.edge xwf.id={1} parent.task.id="{2}" child.task.id="{3}"'
        
        self.job_edge_text = 'ts={0} event=job.edge  xwf.id={1} parent.job.id="{2}" child.job.id="{3}"'
        
    def form_task_id (self, job_id):
        return "{0}{1}".format (job_id, "Task")

    def emit_job_info (self, wf_uuid, job_id, properties):
        task_id = self.form_task_id (job_id)
        self.task.append (self.task_text.format (self.u.get_timestamp (), wf_uuid, task_id, self.wf))
        self.job.append (self.job_text.format (self.u.get_timestamp (), wf_uuid, job_id,
                                               properties ['submit_file'],
                                               properties ['executable'],
                                               '')) #properties ['arguments']))
        self.job_map.append (self.map_text.format (self.u.get_timestamp (), wf_uuid, job_id, task_id))

    def emit_edge_info (self, wf_uuid, parent_job_id, child_job_id):
        self.task_edge.append (self.task_edge_text.format (self.u.get_timestamp (), wf_uuid, 
                                                           self.form_task_id (parent_job_id),
                                                           self.form_task_id (child_job_id)))
        self.job_edge.append (self.job_edge_text.format (self.u.get_timestamp (), wf_uuid,
                                                         parent_job_id,
                                                         child_job_id))
    def write (self):
        output_file = self.dag.replace (".dag", ".static.bp")
        logger.info ("Writing statbp: %s", output_file)
        with open (output_file, 'w') as stream:
            for section in [ self.task, self.task_edge, self.job, self.job_map, self.job_edge ]:                    
                for line in section:
                    stream.write ('{0}\n'.format (line))

class DAGParser (object):
    ''' Parses DAGMan files '''
    def __init__(self, dag, dagdir):
        self.dag = dag
        self.dagdir = dagdir

    def parse (self, job_cb=None, parent_cb=None, retry_cb=None, subdag_cb=None):
        jobs = {}
        dagpath = os.path.join (self.dagdir, self.dag)
        with open (dagpath, 'r') as stream:
            for line in stream:
                elem = line.split ()
                if len (elem) == 0:
                    continue
                command = elem [0]
                if command == 'JOB':
                    job_id = elem [1]
                    submitfile = elem [2]
                    if job_cb:
                        job_cb (job_id, submitfile)
                elif command == 'PARENT':
                    parent_job = elem [1]
                    child_job = elem [3]
                    if parent_cb:
                        parent_cb (parent_job, child_job)
        return jobs

class Job (object):
    ''' Represents a job '''
    def __init__ (self, name, submitfile, dagdir):
        self.u = Util ()
        self.sub = submitfile
        self.name = name
        self.dagdir = dagdir
        input = os.path.join (dagdir, submitfile)
        self.properties = self.u.get_file_as_dict (input)

    def get_meta (self):
        return {
            'name'   : self.name,
            'sub'    : self.sub,
            'output' : os.path.basename (self.properties ['output']),
            'error'  : os.path.basename (self.properties ['error'])
            }

class DAGDebug (object):
    ''' Bridge between raw DAGMan and Pegasus STAMPEDE '''
    def __init__(self):
        self.u = Util ()

    def prepare (self, dir_name):
        for root, dirnames, filenames in os.walk (dir_name):
            for idx, file_name in enumerate (fnmatch.filter (filenames, '*.dag')):
                dag_file = os.path.join (root, file_name)
                self.write_statbp (dag_file)
                self.write_braindump (dag_file)
                dirname = os.path.dirname (dag_file)
                self.u.rm_all (dirname, "*.stampede.db")

    def write_statbp (self, dag):
        dagdir = os.path.dirname (dag)
        wf_uuid = os.path.basename (dagdir)    
        statbp = StatBP (dag)
        dag = os.path.basename (dag)
        parser = DAGParser (dag, dagdir)
        def job_cb (job_id, submitfile):
            sub_file = os.path.join (dagdir, submitfile)
            submit_properties = self.u.get_file_as_dict (sub_file)
            submit_properties ['submit_file'] = submitfile
            statbp.emit_job_info (wf_uuid, job_id, submit_properties)
        def parent_cb (parent_job_id, child_job_id):
            statbp.emit_edge_info (wf_uuid, parent_job_id, child_job_id)
        parser.parse (job_cb, parent_cb)
        statbp.write ()

    def write_braindump (self, dag):
        dirname = os.path.dirname (dag)
        braindump = os.path.join (dirname, 'braindump.txt')
        logger.info ("Writing braindump: %s", braindump)
        braindump_text = """dax {1}.dax
dax_label {1}
dax_index 0
dax_version 1.0
wf_uuid {0}
dag {1}
planner_arguments "" """
        with open (braindump, "w") as stream:
            wf_uuid = os.path.basename (os.path.dirname (dag))
            stream.write (braindump_text.format (wf_uuid, os.path.basename (dag).replace ('.dag', '')))

    def monitor (self):
        for root, dirnames, filenames in os.walk ("."):
            for idx, dagmanout in enumerate (fnmatch.filter (filenames, '*.dagman.out')):
                logger.info ("Scanning %s @ %s", dagmanout, root)
                dagdir = root
                output = os.path.join (dagdir, 'output')
                if not os.path.exists (output):
                    os.makedirs (output)
                curdir = os.getcwd ()
                os.chdir (dagdir)
                #command = 'pegasus-monitord {0} {1} --no-daemon --db-stats --output-dir=output --no-notifications'
                command = 'pegasus-monitord {0} {1} --db-stats --output-dir=output --no-notifications'
                command = command.format (dagmanout, '-v -v -v -v')
                logger.info ("Running %s", command)
                os.system (command)
                os.chdir (curdir)

    def write_output (self):
        dagmetas = []
        for root, dirnames, filenames in os.walk ("."):
            for idx, jobstate_log in enumerate (fnmatch.filter (filenames, '*-jobstate.log')):
                logger.info ("Scanning %s @ %s", jobstate_log, root)
                dagdir = os.path.dirname (root)
                dag_p = os.path.join (dagdir, '*.dag')
                dag = os.path.basename (glob.glob (dag_p)[0])
                dagout = '{0}.dagman.out'.format (dag)
                logger.info ('dagdir: %s %s', dagdir, dag)
                
                jobs = {}
                parser = DAGParser (dag, dagdir)
                def job_cb (job_id, submitfile):
                    jobs [job_id] = Job (job_id, submitfile, dagdir)        
                parser.parse (job_cb)

                dagmetas.append (self.get_dagmeta (dag, dagdir, dagout, jobstate_log, jobs))
        self.u.write_json (dagmetas, 'dagmeta.js', preamble = 'var dagmeta = ')

    def get_dagmeta (self, dag, dagdir, dagout, jobstate_log, jobs):
        jobmeta = {}
        for key in jobs:
            job = jobs [key]
            jobmeta [key] = job.get_meta ()
        return {
            'jobstate' : jobstate_log,
            'dag'      : dag,
            'dagdir'   : os.path.abspath (dagdir),
            'dagout'   : dagout,
            'jobs'     : jobmeta
            }

    def main (self):
        ''' Parse arguments. '''
        parser = argparse.ArgumentParser ()
        parser.add_argument ("-p", "--prep",        help="Prepare.", action='store_true', default=True)
        parser.add_argument ("-m", "--monitor",     help="Monitor.", action='store_true', default=False)
        parser.add_argument ("-w", "--write",       help="Write.",   action='store_true', default=False)
        parser.add_argument ("-l", "--loglevel",    help="Log level.", default="error")
        args = parser.parse_args ()
        
        numeric_level = getattr (logging, args.loglevel.upper (), None)
        assert isinstance (numeric_level, int), "Undefined log level: %s" % args.loglevel
        logging.basicConfig (level=numeric_level, format='%(asctime)-15s %(message)s')
        
        if args.prep:
            self.prepare ('.')
        if args.monitor:
            self.monitor ()
        if args.write:
            self.write_output ()

app = DAGDebug ()
app.main ()


