#!/usr/bin/env python

import glob
import string
import os
import sys
import argparse
import logging
import fnmatch 
import string
import json

from datetime import datetime

logger = logging.getLogger (__name__)

def main ():
    ''' Parse arguments. '''
    parser = argparse.ArgumentParser ()
    parser.add_argument ("-p", "--prep",        help="Prepare.", action='store_true', default=True)
    parser.add_argument ("-m", "--monitor",     help="Monitor.", action='store_true', default=False)
    parser.add_argument ("-w", "--write",       help="Write.",   action='store_true', default=True)
    parser.add_argument ("-l", "--loglevel",    help="Log level.", default="error")
    args = parser.parse_args ()

    numeric_level = getattr (logging, args.loglevel.upper (), None)
    assert isinstance (numeric_level, int), "Undefined log level: %s" % args.loglevel
    logging.basicConfig (level=numeric_level, format='%(asctime)-15s %(message)s')

    if args.prep:
        prepare ('.')
    if args.monitor:
        monitor ()
    if args.write:
        write_output ()

def get_file_as_dict (file_name):
    d = {}
    with open (file_name) as f:
        for line in f:
            line = line.strip ()
            if '=' in line:
                (key, val) = string.split (line, sep = '=', maxsplit = 1)
                d [key.strip ()] = val.strip ()
    return d

def get_timestamp ():
    return datetime.now ().strftime ("%FT%H:%M:%SZ")

def prepare (dir_name):
    for root, dirnames, filenames in os.walk (dir_name):
        for idx, file_name in enumerate (fnmatch.filter (filenames, '*.dag')):
            dag_file = os.path.join (root, file_name)
            write_statbp (dag_file)
            write_braindump (dag_file)
    dirname = os.path.dirname (dag_file)
    old_databases = os.path.join (dirname, "*.stampede.db")
    for file_name in glob.glob (old_databases):
        logger.info ("Removing %s", file_name)
        os.remove (file_name)

class StatBP (object):
    def __init__ (self, dag):
        self.dag = dag
        logger.info ("statbp: %s", self.dag)
        self.wf = self.dag.replace ('.dag', '')
        self.task = []
        self.task_edge = []
        self.job = []
        self.job_edge = []
        self.job_map = []        
            
        self.task_text = ''.join ([ 'ts={0} event=task.info xwf.id={1} task.id="{2}" type="1" ',
                                    'type_desc="compute" transformation="{3}::{2}:1.0" argv=""' ])
        
        self.job_text = ''.join ([ 'ts={0} event=job.info xwf.id={1} job.id="{2}" type="1" submit_file="{3}" ',
                                   'type_desc="compute" clustered="0" max_retries="3" ',
                                   'executable="{4}" argv="{5}" task_count="0"' ])
        
        self.map_text = 'ts={0} event=wf.map.task_job xwf.id={1} job.id="{2}" task.id="{3}"'
            
        self.task_edge_text = 'ts={0} event=task.edge xwf.id={1} parent.task.id="{2}" child.task.id="{3}"'
        
        self.job_edge_text = 'ts={0} event=job.edge  xwf.id={1} parent.job.id="{2}" child.job.id="{3}"'
        
    def form_task_id (self, job_id):
        return "{0}{1}".format (job_id, "Task")

    def emit_job_info (self, wf_uuid, job_id, properties):
        task_id = self.form_task_id (job_id)
        self.task.append (self.task_text.format (get_timestamp (), wf_uuid, task_id, self.wf))
        self.job.append (self.job_text.format (get_timestamp (), wf_uuid, job_id,
                                               properties ['submit_file'],
                                               properties ['executable'],
                                               '')) #properties ['arguments']))
        self.job_map.append (self.map_text.format (get_timestamp (), wf_uuid, job_id, task_id))

    def emit_edge_info (self, wf_uuid, parent_job_id, child_job_id):
        self.task_edge.append (self.task_edge_text.format (get_timestamp (), wf_uuid, 
                                                           self.form_task_id (parent_job_id),
                                                           self.form_task_id (child_job_id)))
        self.job_edge.append (self.job_edge_text.format (get_timestamp (), wf_uuid,
                                                         parent_job_id,
                                                         child_job_id))
    def write (self):
        output_file = self.dag.replace (".dag", ".static.bp")
        logger.info ("Writing statbp: %s", output_file)
        with open (output_file, 'w') as stream:
            for section in [ self.task, self.task_edge, self.job, self.job_map, self.job_edge ]:                    
                for line in section:
                    stream.write ('{0}\n'.format (line))

def write_statbp (dag):
    dagdir = os.path.dirname (dag)
    wf_uuid = os.path.basename (dagdir)    
    statbp = StatBP (dag)

    dag = os.path.basename (dag)
    parser = DAGParser (dag, dagdir)

    def job_cb (job_id, submitfile):
        sub_file = os.path.join (dagdir, submitfile)
        submit_properties = get_file_as_dict (sub_file)
        submit_properties ['submit_file'] = submitfile
        statbp.emit_job_info (wf_uuid, job_id, submit_properties)
    def parent_cb (parent_job_id, child_job_id):
        statbp.emit_edge_info (wf_uuid, parent_job_id, child_job_id)

    parser.parse (job_cb, parent_cb)
    statbp.write ()

def write_braindump (dag):
    dirname = os.path.dirname (dag)
    braindump = os.path.join (dirname, 'braindump.txt')
    logger.info ("Writing braindump: %s", braindump)
    braindump_text = """wf_uuid {0}
dag {1}
planner_arguments "" """
    with open (braindump, "w") as stream:
        wf_uuid = os.path.basename (os.path.dirname (dag))
        stream.write (braindump_text.format (wf_uuid, os.path.basename (dag).replace ('.dag', '')))

def monitor ():
    for root, dirnames, filenames in os.walk ("."):
        for idx, dagmanout in enumerate (fnmatch.filter (filenames, '*.dagman.out')):
            logger.info ("Scanning %s @ %s", dagmanout, root)
            dagdir = root
            output = os.path.join (dagdir, 'output')
            if not os.path.exists (output):
                os.makedirs (output)
            dagmanoutbase = os.path.basename (dagmanout)
            curdir = os.getcwd ()
            os.chdir (dagdir)
            logger.info ("Instrumenting %s", dagdir)
            command = 'pegasus-monitord {0} {1} --no-daemon --db-stats --output-dir=output --no-notifications'
            command = command.format (dagmanout, '-v -v -v -v')
            logger.info ("Running %s", command)
            os.system (command)
            os.chdir (curdir)

def write_output ():
    dagmetas = []
    for root, dirnames, filenames in os.walk ("."):
        for idx, jobstate_log in enumerate (fnmatch.filter (filenames, '*-jobstate.log')):
            logger.info ("Scanning %s @ %s", jobstate_log, root)
            dagdir = os.path.dirname (root)
            dag_p = os.path.join (dagdir, '*.dag')
            dag = os.path.basename (glob.glob (dag_p)[0])
            dagout = '{0}.dagman.out'.format (dag)
            logger.info ('dagdir: %s %s', dagdir, dag)
            jobs = parse_dag (dag, dagdir)
            dagmetas.append (get_dagmeta (dag, dagdir, dagout, jobstate_log, jobs))
    write_json (dagmetas, 'dagmeta.js', preamble = 'var dagmeta = ')

def write_json (obj, file_name, preamble = ''):
    with open (file_name, 'w') as stream:
        stream.write (preamble)
        stream.write (json.dumps (obj, indent = 2, sort_keys = True))

def parse_dag (dag, dagdir):
    jobs = {}
    parser = DAGParser (dag, dagdir)
    def job_cb (job_id, submitfile):
        jobs [job_id] = Job (job_id, submitfile, dagdir)        
    parser.parse (job_cb)
    return jobs

class DAGParser (object):
    def __init__(self, dag, dagdir):
        self.dag = dag
        self.dagdir = dagdir

    def parse (self, job_cb=None, parent_cb=None, retry_cb=None, subdag_cb=None):
        jobs = {}
        dagpath = os.path.join (self.dagdir, self.dag)
        with open (dagpath, 'r') as stream:
            for line in stream:
                elem = line.split ()
                if len (elem) == 0:
                    continue
                command = elem [0]
                if command == 'JOB':
                    job_id = elem [1]
                    submitfile = elem [2]
                    if job_cb:
                        job_cb (job_id, submitfile)
                elif command == 'PARENT':
                    parent_job = elem [1]
                    child_job = elem [3]
                    if parent_cb:
                        parent_cb (parent_job, child_job)
        return jobs

class Job (object):

    def __init__ (self, name, submitfile, dagdir):
        self.sub = submitfile
        self.name = name
        self.dagdir = dagdir
        input = os.path.join (dagdir, submitfile)
        self.properties = get_file_as_dict (input)

    def get_meta (self):
        return {
            'name'   : self.name,
            'sub'    : self.sub,
            'output' : os.path.basename (self.properties ['output']),
            'error'  : os.path.basename (self.properties ['error'])
            }
          
def get_dagmeta (dag, dagdir, dagout, jobstate_log, jobs):
    jobmeta = {}
    for key in jobs:
        job = jobs [key]
        jobmeta [key] = job.get_meta ()
    return {
        'jobstate' : jobstate_log,
        'dag'      : dag,
        'dagdir'   : os.path.abspath (dagdir),
        'dagout'   : dagout,
        'jobs'     : jobmeta
        }
    
main ()


